# ğŸ•µï¸ SniffrAI

**SniffrAI** is an intelligent web scraping and data extraction tool that uses AI to automatically classify, extract, and store structured data from websites. It combines web scraping, LLM-powered data processing, and automatic database management.

## ğŸš€ Features

- **ğŸ¤– AI-Powered Classification**: Automatically classifies data based on content and URLs
- **ğŸ” Multi-Tool Extraction**: Supports Google Search, FireCrawl, and custom scrapers
- **ğŸ“Š Dynamic Schema Generation**: Creates Pydantic models on-the-fly based on data structure
- **ğŸ—„ï¸ Automatic Database Management**: Creates tables and handles data storage automatically
- **ğŸ†” Smart Primary Key Handling**: Generates UUIDs for missing primary keys
- **ğŸ”„ Data Refinement**: AI-powered data cleaning and structuring
- **ğŸ“ˆ Batch Processing**: Handles multiple records efficiently
- **ğŸ›¡ï¸ Error Handling**: Comprehensive error handling and logging

## ğŸ—ï¸ Architecture

```
SniffrAI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ sniffer.py          # Main API endpoint
â”‚   â”‚   â”‚   â””â”€â”€ scrape_lenders.py   # Specialized scrapers
â”‚   â”‚   â””â”€â”€ routes.py               # API routing
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ logger.py               # Logging configuration
â”‚   â”‚   â””â”€â”€ settings.py             # Application settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py              # Pydantic models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ crawlers.py             # Web crawling services
â”‚   â”‚   â”œâ”€â”€ database_service.py     # Database operations
â”‚   â”‚   â”œâ”€â”€ gemini_service.py       # Google Gemini integration
â”‚   â”‚   â”œâ”€â”€ llm_services.py         # LLM processing
â”‚   â”‚   â””â”€â”€ sniffer_services.py     # Core sniffer logic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ prompts.py              # AI prompts
â”‚       â””â”€â”€ validators.py           # Data validation
â”œâ”€â”€ config.yaml                     # Configuration file
â””â”€â”€ requirements.txt                # Python dependencies
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- PostgreSQL database (via Supabase)
- API keys for:
  - OpenAI
  - Google Gemini
  - FireCrawl
  - Supabase

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd SniffrAI
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Environment Configuration**
   Create a `.env` file with your API keys:
   ```env
   OPENAI_API_KEY=your_openai_key
   GEMINI_API_KEY=your_gemini_key
   FIRECRAWL_API_KEY=your_firecrawl_key
   SUPABASE_URL=your_supabase_url
   SUPABASE_SERVICE_ROLE_KEY=your_supabase_key
   ```

5. **Configure Settings**
   Update `config.yaml` with your use cases and configurations.

6. **Run the Application**
   ```bash
   uvicorn app.main:app --reload
   ```

## ğŸ“– Usage

### Basic API Request

```python
import requests

# Basic scraping request
data = {
    "urls": ["https://example-bank.com/home-loans"],
    "snifferTool": True,
    "enableRefinement": True
}

response = requests.post("http://localhost:8000/api/sniffer_ai", json=data)
print(response.json())
```

### Request Parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| `urls` | List[str] | URLs to scrape (required) |
| `snifferTool` | bool | Enable FireCrawl scraping |
| `googleSearch` | bool | Enable Google search |
| `enableRefinement` | bool | Enable AI data refinement |
| `prompt` | str | Additional context for scraping |

### Response Format

```json
{
    "success": true,
    "message": "Data extraction completed successfully"
}
```

## ğŸ”§ Configuration

### Use Cases Configuration (`config.yaml`)

```yaml
use_cases:
  lenders:
    keywords: ["interest", "roi", "loan", "bank"]
    table_name: "lenders_data"
    primary_key: "lender"
    output_format: "LendersGeminiSearchResponse"
    scraper_system_message: "lenders_system_message"
    scraper_prompt: "lenders_scraper_prompt"
    refinement_prompt: "lenders_refinement_prompt"
    
  doctors:
    keywords: ["doctor", "clinic", "medical"]
    table_name: "doctors_data"
    primary_key: "name"
    output_format: "SnifferExtractSchema"
```

### Adding New Use Cases

1. **Add to config.yaml**
2. **Create corresponding Pydantic schema in `models/schemas.py`**
3. **Add schema mapping in `api/endpoints/sniffer.py`**

## ğŸ—„ï¸ Database Schema

SniffrAI automatically creates tables with the following structure:

```sql
CREATE TABLE your_table (
    id TEXT PRIMARY KEY,                    -- Auto-generated UUID
    your_unique_field TEXT UNIQUE,         -- Business unique field
    data_field_1 TEXT,
    data_field_2 TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### Key Features:
- **Fixed Primary Key**: Always `id` with UUID
- **Unique Constraints**: Business fields marked as unique
- **Automatic Timestamps**: `created_at` and `updated_at`
- **UUID Generation**: Automatic for missing primary keys

## ğŸ¤– AI Components

### 1. Classification Agent
Automatically classifies incoming requests based on URLs and content.

### 2. Config Generation Agent
Generates dynamic configurations for new use cases.

### 3. Data Extraction
Uses FireCrawl and custom scrapers for data extraction.

### 4. Refinement Agent
Cleans and structures extracted data using LLMs.

## ğŸ“Š Supported Data Sources

- **Financial Institutions**: Banks, lenders, loan providers
- **Healthcare**: Doctors, clinics, medical services  
- **Professional Services**: Consultants, advisors
- **Custom**: Configurable for any domain

## ğŸ” Example Use Cases

### 1. Lender Data Extraction
```python
{
    "urls": ["https://bank.com/home-loans"],
    "snifferTool": True,
    "enableRefinement": True
}
```

**Extracts**: Interest rates, loan terms, eligibility criteria, etc.

### 2. Doctor Directory Scraping
```python
{
    "urls": ["https://medical-directory.com"],
    "snifferTool": True
}
```

**Extracts**: Doctor names, specializations, contact info, etc.

### 3. Google Search Integration
```python
{
    "urls": ["example.com"],
    "googleSearch": True,
    "prompt": "Find loan information"
}
```

**Searches**: Google for relevant information and extracts data.

## ğŸ›¡ï¸ Error Handling

- **Validation**: Input validation for all requests
- **Retry Logic**: Automatic retries for failed operations
- **Logging**: Comprehensive logging for debugging
- **Graceful Degradation**: Fallback mechanisms for failures

## ğŸ“ˆ Performance

- **Batch Processing**: Handles multiple records efficiently
- **Async Operations**: Non-blocking I/O operations
- **Caching**: Intelligent caching of responses
- **Rate Limiting**: Respects API rate limits

## ğŸ”’ Security

- **API Key Management**: Secure handling of API keys
- **Input Sanitization**: Prevents injection attacks
- **CORS Configuration**: Proper CORS setup
- **Environment Isolation**: Separate environments for dev/prod

## ğŸ“ API Documentation

### Endpoints

#### `POST /api/sniffer_ai`
Main endpoint for data extraction and processing.

**Request Body:**
```json
{
    "urls": ["string"],
    "snifferTool": boolean,
    "googleSearch": boolean,
    "enableRefinement": boolean,
    "prompt": "string"
}
```

**Response:**
```json
{
    "success": boolean,
    "message": "string"
}
```

## ğŸ§ª Testing

Run tests using:
```bash
python -m pytest tests/
```

Test files are located in `app/testing/` directory.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue on GitHub
- Check the documentation
- Review the logs for debugging

## ğŸ”„ Changelog

### v1.0.0
- Initial release
- Basic web scraping functionality
- AI-powered classification
- Automatic database management
- UUID primary key generation

---

**Built with â¤ï¸ using FastAPI, Pydantic, and AI**
